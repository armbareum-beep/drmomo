import os
from dotenv import load_dotenv
from langchain_community.document_loaders import (
    PyPDFLoader,
    Docx2txtLoader,
    TextLoader,
    CSVLoader,
    UnstructuredExcelLoader,
    UnstructuredHTMLLoader,
    UnstructuredMarkdownLoader
)
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_core.prompts import PromptTemplate
from langchain.chains import RetrievalQA

# Load environment variables
load_dotenv()

def get_loader_for_file(file_path):
    """
    Returns the appropriate document loader based on file extension.
    """
    ext = os.path.splitext(file_path)[1].lower()
    
    loaders = {
        '.pdf': PyPDFLoader,
        '.docx': Docx2txtLoader,
        '.doc': Docx2txtLoader,
        '.txt': TextLoader,
        '.md': UnstructuredMarkdownLoader,
        '.csv': CSVLoader,
        '.xlsx': UnstructuredExcelLoader,
        '.xls': UnstructuredExcelLoader,
        '.html': UnstructuredHTMLLoader,
        '.htm': UnstructuredHTMLLoader,
    }
    
    loader_class = loaders.get(ext)
    if loader_class:
        return loader_class(file_path)
    else:
        print(f"Warning: Unsupported file type: {ext}")
        return None

def load_and_process_documents(file_paths=None, folder_path=None):
    """
    Loads documents from specified files or folder, splits them into chunks, and creates a vector store.
    Supports: PDF, DOCX, DOC, TXT, MD, CSV, XLSX, XLS, HTML
    """
    documents = []
    
    # Collect all file paths
    all_paths = []
    
    if file_paths:
        all_paths.extend(file_paths)
    
    if folder_path and os.path.isdir(folder_path):
        for filename in os.listdir(folder_path):
            file_path = os.path.join(folder_path, filename)
            if os.path.isfile(file_path):
                all_paths.append(file_path)
    
    # Load documents
    for path in all_paths:
        if os.path.exists(path):
            loader = get_loader_for_file(path)
            if loader:
                try:
                    documents.extend(loader.load())
                    print(f"âœ“ Loaded: {os.path.basename(path)}")
                except Exception as e:
                    print(f"âœ— Error loading {os.path.basename(path)}: {str(e)}")
        else:
            print(f"Warning: File not found: {path}")

    if not documents:
        raise ValueError("No documents loaded. Please add files to the 'documents' folder.")

    # Text Splitting
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=150
    )
    texts = text_splitter.split_documents(documents)
    
    print(f"\nğŸ“š Total documents loaded: {len(documents)}")
    print(f"ğŸ“ Total text chunks created: {len(texts)}")

    # Embeddings & Vector Store
    embeddings = OpenAIEmbeddings()
    vectorstore = FAISS.from_documents(texts, embeddings)
    
    return vectorstore

def create_rag_chain(vectorstore):
    """
    Creates a RetrievalQA chain with the specific veterinary prompt.
    """
    llm = ChatOpenAI(model_name="gpt-4o", temperature=0) # Using gpt-4o for better reasoning

    # Define the prompt template based on user requirements
    prompt_template = """ìˆ˜ì˜í•™RAG

ë‹¹ì‹ ì€ ìˆ˜ì˜í•™ ì „ë¬¸ ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

ì•„ë˜ì˜ "ì»¨í…ìŠ¤íŠ¸"ëŠ” ì‹ ë¢° ê°€ëŠ¥í•œ ìˆ˜ì˜í•™ ìë£Œ(PDF, ê°€ì´ë“œë¼ì¸, êµê³¼ì„œ, ë…¼ë¬¸)ì—ì„œ 
RAG ê²€ìƒ‰ì„ í†µí•´ ê°€ì ¸ì˜¨ ë‚´ìš©ì…ë‹ˆë‹¤. 

ë‹¹ì‹ ì˜ ê·œì¹™ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

1) ë°˜ë“œì‹œ "ì»¨í…ìŠ¤íŠ¸" ì•ˆì˜ ì •ë³´ë§Œ ì‚¬ìš©í•´ ë‹µí•˜ì„¸ìš”.
2) ì»¨í…ìŠ¤íŠ¸ì—ì„œ í™•ì¸ë˜ì§€ ì•ŠëŠ” ë‚´ìš©ì€ ì ˆëŒ€ ì¶”ì¸¡í•˜ê±°ë‚˜ ë§Œë“¤ì–´ë‚´ì§€ ë§ˆì„¸ìš”.
3) ëª¨ë¥´ë©´ "í•´ë‹¹ ì •ë³´ëŠ” ì œê³µëœ ìë£Œì—ì„œ í™•ì¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ë‹µí•˜ì„¸ìš”.
4) ì•½ë¬¼ ìš©ëŸ‰, íˆ¬ì•½ ê°„ê²©, ê¸ˆê¸°ì‚¬í•­ì€ ë°˜ë“œì‹œ ì»¨í…ìŠ¤íŠ¸ ê·¼ê±°ë¥¼ ê¸°ë°˜ìœ¼ë¡œë§Œ ë§í•˜ì„¸ìš”.
5) ì‘ê¸‰ ìƒí™©(ë°œì‘, í˜¸í¡ê³¤ë€, ì‡¼í¬ ë“±)ì´ë©´ ì¦‰ì‹œ ì‘ê¸‰ ë™ë¬¼ë³‘ì› ë°©ë¬¸ ê¶Œê³ ë¥¼ í¬í•¨í•˜ì„¸ìš”.
6) í•­ìƒ ìµœì¢… ë¬¸ì¥ì— ë‹¤ìŒ ê²½ê³ ë¬¸ì„ í¬í•¨í•˜ì„¸ìš”:
   "âš ï¸ ì´ ì •ë³´ëŠ” ì°¸ê³ ìš©ì´ë©°, ì‹¤ì œ ì§„ë‹¨Â·ì²˜ë°©ì€ ë°˜ë“œì‹œ ìˆ˜ì˜ì‚¬ê°€ ì§ì ‘ íŒë‹¨í•´ì•¼ í•©ë‹ˆë‹¤."

[ì»¨í…ìŠ¤íŠ¸]
{context}

[ì§ˆë¬¸]
{question}

ìœ„ ê·œì¹™ì„ ì§€ì¼œì„œ í•œêµ­ì–´ë¡œ ëª…í™•í•˜ê³  êµ¬ì¡°ì ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”:

- ìš”ì•½ ë‹µë³€
- ì¹˜ë£Œ/ì˜í•™ì  ì„¤ëª…(ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜)
- ì£¼ì˜ì‚¬í•­ ë˜ëŠ” ê¸ˆê¸°
- ì¶œì²˜(ì–´ë–¤ ë¬¸ì„œ/í˜ì´ì§€ì—ì„œ ë‚˜ì˜¨ ì •ë³´ì¸ì§€)
"""
    
    PROMPT = PromptTemplate(
        template=prompt_template, input_variables=["context", "question"]
    )

    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=vectorstore.as_retriever(),
        chain_type_kwargs={"prompt": PROMPT},
        return_source_documents=True 
    )

    return qa_chain
