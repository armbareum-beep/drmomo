import os
from dotenv import load_dotenv
from langchain_community.document_loaders import (
    PyPDFLoader,
    Docx2txtLoader,
    TextLoader,
    CSVLoader,
    UnstructuredExcelLoader,
    UnstructuredHTMLLoader,
    UnstructuredMarkdownLoader
)
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_pinecone import PineconeVectorStore
from pinecone import Pinecone, ServerlessSpec
import time
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_core.prompts import PromptTemplate
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain

# Load environment variables
load_dotenv()

def get_loader_for_file(file_path):
    """
    Returns the appropriate document loader based on file extension.
    """
    ext = os.path.splitext(file_path)[1].lower()
    
    loaders = {
        '.pdf': PyPDFLoader,
        '.docx': Docx2txtLoader,
        '.doc': Docx2txtLoader,
        '.txt': TextLoader,
        '.md': UnstructuredMarkdownLoader,
        '.csv': CSVLoader,
        '.xlsx': UnstructuredExcelLoader,
        '.xls': UnstructuredExcelLoader,
        '.html': UnstructuredHTMLLoader,
        '.htm': UnstructuredHTMLLoader,
    }
    
    loader_class = loaders.get(ext)
    if loader_class:
        return loader_class(file_path)
    else:
        print(f"Warning: Unsupported file type: {ext}")
        return None

def load_and_process_documents(file_paths=None, folder_path=None):
    """
    Loads documents from specified files or folder, splits them into chunks, and creates a vector store.
    Supports: PDF, DOCX, DOC, TXT, MD, CSV, XLSX, XLS, HTML
    
    Improved with:
    - Better error handling for corrupted PDFs
    - Progress tracking
    - Skips problematic files instead of crashing
    """
    documents = []
    
    # Collect all file paths
    all_paths = []
    
    if file_paths:
        all_paths.extend(file_paths)
    
    if folder_path and os.path.isdir(folder_path):
        for filename in os.listdir(folder_path):
            file_path = os.path.join(folder_path, filename)
            if os.path.isfile(file_path):
                all_paths.append(file_path)
    
    # Load documents with better error handling
    successful_files = []
    failed_files = []
    
    print(f"\nğŸ“‚ Found {len(all_paths)} files to process...")
    
    for idx, path in enumerate(all_paths, 1):
        if os.path.exists(path):
            filename = os.path.basename(path)
            print(f"[{idx}/{len(all_paths)}] Processing: {filename}...", end=" ")
            
            loader = get_loader_for_file(path)
            if loader:
                try:
                    # Load with timeout protection
                    loaded_docs = loader.load()
                    
                    if loaded_docs:
                        documents.extend(loaded_docs)
                        successful_files.append(filename)
                        print(f"âœ“ ({len(loaded_docs)} pages)")
                    else:
                        print(f"âš ï¸ Empty file")
                        
                except Exception as e:
                    error_msg = str(e)
                    # Shorten long error messages
                    if len(error_msg) > 100:
                        error_msg = error_msg[:100] + "..."
                    print(f"âœ— Error: {error_msg}")
                    failed_files.append((filename, str(e)))
            else:
                print(f"âš ï¸ Unsupported format")
        else:
            print(f"Warning: File not found: {path}")

    # Summary
    print(f"\n{'='*60}")
    print(f"âœ… Successfully loaded: {len(successful_files)} files")
    if failed_files:
        print(f"âŒ Failed to load: {len(failed_files)} files")
        print(f"\nFailed files:")
        for filename, error in failed_files[:5]:  # Show first 5
            print(f"  - {filename}")
        if len(failed_files) > 5:
            print(f"  ... and {len(failed_files) - 5} more")
    print(f"{'='*60}\n")

    if not documents:
        raise ValueError("No documents loaded successfully. Please check your PDF files or add supported documents to the 'documents' folder.")

    # Text Splitting
    print("ğŸ“ Splitting documents into chunks...")
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=150
    )
    texts = text_splitter.split_documents(documents)
    
    print(f"ğŸ“š Total documents loaded: {len(documents)}")
    print(f"ğŸ“ Total text chunks created: {len(texts)}")

    # Embeddings & Vector Store with Pinecone
    print("ğŸ”„ Connecting to Pinecone...")
    embeddings = OpenAIEmbeddings()
    
    # Initialize Pinecone
    pc = Pinecone(api_key=os.environ.get("PINECONE_API_KEY"))
    index_name = "veterinary-rag"
    
    # Check if index exists, create if not
    existing_indexes = [index_info["name"] for index_info in pc.list_indexes()]
    if index_name not in existing_indexes:
        print(f"Creating new Pinecone index: {index_name}...")
        pc.create_index(
            name=index_name,
            dimension=1536,
            metric="cosine",
            spec=ServerlessSpec(
                cloud="aws",
                region="us-east-1"
            )
        )
        # Wait for index to be ready
        while not pc.describe_index(index_name).status['ready']:
            time.sleep(1)
        print("âœ… Index created and ready!")
    else:
        print(f"âœ… Found existing Pinecone index: {index_name}")

    # Process in batches to avoid API limits and timeouts
    batch_size = 100
    total_batches = (len(texts) + batch_size - 1) // batch_size
    
    print(f"ğŸ“¦ Uploading {len(texts)} chunks to Pinecone in {total_batches} batches...")
    
    # Initialize vectorstore
    vectorstore = PineconeVectorStore(index_name=index_name, embedding=embeddings)
    
    for i in range(0, len(texts), batch_size):
        batch_num = (i // batch_size) + 1
        batch = texts[i:i + batch_size]
        
        print(f"[Batch {batch_num}/{total_batches}] Uploading {len(batch)} chunks...", end=" ")
        
        try:
            vectorstore.add_documents(batch)
            print("âœ“")
        except Exception as e:
            print(f"âœ— Error: {str(e)[:100]}")
            # Continue with next batch
            continue
    
    print("âœ… All batches uploaded successfully!")
    return vectorstore



def create_rag_chain(vectorstore):
    """
    Creates a retrieval chain with the specific veterinary prompt.
    """
    llm = ChatOpenAI(model_name="gpt-4o", temperature=0)

    # Define the prompt template
    prompt_template = """ìˆ˜ì˜í•™RAG

ë‹¹ì‹ ì€ ìˆ˜ì˜í•™ ì „ë¬¸ ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

ì•„ë˜ì˜ "ì»¨í…ìŠ¤íŠ¸"ëŠ” ì‹ ë¢° ê°€ëŠ¥í•œ ìˆ˜ì˜í•™ ìë£Œ(PDF, ê°€ì´ë“œë¼ì¸, êµê³¼ì„œ, ë…¼ë¬¸)ì—ì„œ 
RAG ê²€ìƒ‰ì„ í†µí•´ ê°€ì ¸ì˜¨ ë‚´ìš©ì…ë‹ˆë‹¤. 

ë‹¹ì‹ ì˜ ê·œì¹™ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

1) ë°˜ë“œì‹œ "ì»¨í…ìŠ¤íŠ¸" ì•ˆì˜ ì •ë³´ë§Œ ì‚¬ìš©í•´ ë‹µí•˜ì„¸ìš”.
2) ì»¨í…ìŠ¤íŠ¸ì—ì„œ í™•ì¸ë˜ì§€ ì•ŠëŠ” ë‚´ìš©ì€ ì ˆëŒ€ ì¶”ì¸¡í•˜ê±°ë‚˜ ë§Œë“¤ì–´ë‚´ì§€ ë§ˆì„¸ìš”.
3) ëª¨ë¥´ë©´ "í•´ë‹¹ ì •ë³´ëŠ” ì œê³µëœ ìë£Œì—ì„œ í™•ì¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ë‹µí•˜ì„¸ìš”.
4) ì•½ë¬¼ ìš©ëŸ‰, íˆ¬ì•½ ê°„ê²©, ê¸ˆê¸°ì‚¬í•­ì€ ë°˜ë“œì‹œ ì»¨í…ìŠ¤íŠ¸ ê·¼ê±°ë¥¼ ê¸°ë°˜ìœ¼ë¡œë§Œ ë§í•˜ì„¸ìš”.
5) ì‘ê¸‰ ìƒí™©(ë°œì‘, í˜¸í¡ê³¤ë€, ì‡¼í¬ ë“±)ì´ë©´ ì¦‰ì‹œ ì‘ê¸‰ ë™ë¬¼ë³‘ì› ë°©ë¬¸ ê¶Œê³ ë¥¼ í¬í•¨í•˜ì„¸ìš”.
6) í•­ìƒ ìµœì¢… ë¬¸ì¥ì— ë‹¤ìŒ ê²½ê³ ë¬¸ì„ í¬í•¨í•˜ì„¸ìš”:
   "âš ï¸ ì´ ì •ë³´ëŠ” ì°¸ê³ ìš©ì´ë©°, ì‹¤ì œ ì§„ë‹¨Â·ì²˜ë°©ì€ ë°˜ë“œì‹œ ìˆ˜ì˜ì‚¬ê°€ ì§ì ‘ íŒë‹¨í•´ì•¼ í•©ë‹ˆë‹¤."

[ì»¨í…ìŠ¤íŠ¸]
{context}

[ì§ˆë¬¸]
{input}

ìœ„ ê·œì¹™ì„ ì§€ì¼œì„œ í•œêµ­ì–´ë¡œ ëª…í™•í•˜ê³  êµ¬ì¡°ì ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”:

- ìš”ì•½ ë‹µë³€
- ì¹˜ë£Œ/ì˜í•™ì  ì„¤ëª…(ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜)
- ì£¼ì˜ì‚¬í•­ ë˜ëŠ” ê¸ˆê¸°
- ì¶œì²˜(ì–´ë–¤ ë¬¸ì„œ/í˜ì´ì§€ì—ì„œ ë‚˜ì˜¨ ì •ë³´ì¸ì§€)
"""
    
    PROMPT = PromptTemplate(
        template=prompt_template, input_variables=["context", "input"]
    )

    # Create document chain
    document_chain = create_stuff_documents_chain(llm, PROMPT)
    
    # Create retrieval chain
    retrieval_chain = create_retrieval_chain(
        vectorstore.as_retriever(),
        document_chain
    )

    return retrieval_chain
